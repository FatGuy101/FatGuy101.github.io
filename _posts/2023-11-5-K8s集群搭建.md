---
layout: post
title: 使用kubeadm部署Kubernetes集群
subtitle: 单master和多master高可用
categories: markdown
tags: [Kubernetes]
---



## Kubernetes单master集群搭建

### 环境

硬件环境：Centos7，2C2G

| hosts            | ip            |
| ---------------- | ------------- |
| server10(master) | 192.168.25.10 |
| server11         | 192.168.25.11 |
| server12         | 192.168.25.12 |



### 系统整体环境配置，所有节点又要配置

~~~shell
#语言设置
localectl set-locale LANG=en_US.utf8
#epel源
wget -O /etc/yum.repos.d/epel.repo https://mirrors.aliyun.com/repo/epel-7.repo
#设置时区
timedatectl set-timezone Asia/Shanghai
#校正系统时间
yum install -y ntpdate
ntpdate ntp.aliyun.com
#将时间写入本地硬件时钟 RTC
timedatectl set-local-rtc 0
#清除及关闭firewalld,关闭selinux
iptables -F
systemctl  stop firewalld
systemctl disable firewalld
setenforce 0
sed -i '/SELINUX=/c\SELINUX=disabled/' /etc/selinux/config
#关闭swap（因为swap分区使用会影响k8s性能）
swapoff -a
sed -i '/swap/s/.*/#&/' /etc/fstab
#设置内核及相关网络参数
yum install -y ipvsadm ipset
modprobe br_netfilter
modprobe overlay
modprobe ip_vs_sh
modprobe ip_vs_wrr
modprobe ip_vs_rr
modprobe ip_vs

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
ip_vs_sh
ip_vs_wrr
ip_vs_rr
ip_vs
EOF

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
net.ipv4.tcp_tw_recycle=0
vm.swappiness=0
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_instances=8192
fs.inotify.max_user_watches=1048576
fs.file-max=52706963
fs.nr_open=52706963
net.ipv6.conf.all.disable_ipv6=1
net.netfilter.nf_conntrack_max=2310720
EOF

sysctl -p /etc/sysctl.d/k8s.conf
~~~

配置定时任务

~~~shell
crontab  -e

01 */2 * * *  ntpdata   npt.aliyun.com
~~~

~~~shell
systemctl  restart crond.service
~~~

配置A记录解析

~~~shell
cat > /etc/hosts << EOF
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

192.168.25.10  server10
192.168.25.11  server11
192.168.25.12  server12
EOF
~~~

安装docer-cd

官方文档：https://docs.docker.com/engine/install/centos/

~~~shell
#卸载机器上老版本
yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-engine
#配置yum源
yum  install -y yum-utils
yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo
#安装
yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
#docker镜像加速及私有仓库信任
cat > /etc/docker/daemon.json << EOF
{
   "registry-mirrors":["https://hub-mirror.c.163.com/"],
   "insecure-registries":["http://hub.registry.com"]
}
EOF
#启动
systemctl start docker
systemctl enable docker
~~~

配置kubernetes 阿里仓库源

~~~shell
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
~~~

~~~shell
yum clean all
yum makecache fast
~~~

安装cri-dockerd

地址：https://github.com/Mirantis/cri-dockerd

~~~shell
yum install -y go git
~~~

~~~shell
tar -xf /tmp/cri-dockerd.tar.gz
~~~

~~~shell
cd cri-dockerd

make cri-dockerd

install -o root -g root -m 0755 cri-dockerd    /usr/bin/cri-dockerd

 install packaging/systemd/* /usr/lib/systemd/system/
~~~

配置systemd文件

~~~shell
cat > /usr/lib/systemd/system/cri-docker.service << EOF               
[Unit]
Description=CRI Interface for Docker Application Container Engine
Documentation=https://docs.mirantis.com
After=network-online.target firewalld.service docker.service
Wants=network-online.target
Requires=cri-docker.socket
[Service]
Type=notify
ExecStart=/usr/bin/cri-dockerd --container-runtime-endpoint fd:// --network-plugin=cni --cni-bin-dir=/opt/cni/bin --cni-cache-dir=/var/lib/cni/cache --cni-conf-dir=/etc/cni/net.d --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9
ExecReload=/bin/kill -s HUP 
TimeoutSec=0
RestartSec=2
Restart=always
StartLimitBurst=3
StartLimitInterval=60s
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
TasksMax=infinity
Delegate=yes
KillMode=process
[Install]
WantedBy=multi-user.target
EOF
~~~

配置socket文件

~~~shell
cat > /usr/lib/systemd/system/cri-docker.socket  << EOF 
[Unit]
Description=CRI Docker Socket for the API
PartOf=cri-docker.service

[Socket]
ListenStream=%t/cri-dockerd.sock
SocketMode=0660
SocketUser=root
SocketGroup=root

[Install]
WantedBy=sockets.target
EOF
~~~

~~~shell
systemctl daemon-reload
systemctl start cri-docker.service
systemctl enable cri-docker.service
~~~

安装kubeadm工具

~~~shell
#查看kubernetes历史版本
yum list --showduplicates | grep kube
~~~

~~~shell
#安装对应版本
yum install -y kubelet-1.26.3 kubectl-1.26.3 kubeadm-1.26.3
~~~

~~~shell
systemctl enable kubelet
~~~



### master节点操作

初始化集群

~~~shell
kubeadm init --apiserver-advertise-address=192.168.25.10 --kubernetes-version=v1.26.3 --image-repository registry.aliyuncs.com/google_containers --cri-socket unix:///var/run/cri-dockerd.sock --service-cidr=10.1.0.0/16 --pod-network-cidr=10.2.0.0/16 --token-ttl=0
~~~

看到这些则表示初始化成功

~~~shell
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following oneach as root:

kubeadm join 192.168.25.10:6443 --token xg7ibt.8qonzrnjziedn8o6 \
	--discovery-token-ca-cert-hash sha256:bba7097fa4fb06aab523d24f6788e5552a86202fc7a74ed09406fb21915dd87c
~~~

按提示信息建立kubernetes集群配置文件

~~~shell
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
export KUBECONFIG=/etc/kubernetes/admin.conf
~~~



如果初始集群错误，可以清除尝试重新初始化

~~~shell
rm -rf /etc/kubernetes/*
rm -rf ~/.kube/*
rm -rf /var/lib/etcd/*
kubeadm reset
~~~



### 配置kubernetes命令shell  tab补全

官方文档：https://v1-26.docs.kubernetes.io/zh-cn/docs/tasks/tools/install-kubectl-linux/

~~~shell
yum install bash-com*
kubectl completion bash | sudo tee /etc/bash_completion.d/kubectl > /dev/null
chmod a+r /etc/bash_completion.d/kubectl
echo 'alias k=kubectl' >>~/.bashrc
echo 'complete -o default -F __start_kubectl k' >>~/.bashrc
~~~

~~~shell
 #最后
 exit
 #或者
 source
 #重加载shell
~~~



### 子节点

先删除可能影响的因数

~~~shell
rm -rf /var/run/containerd/containerd.sock
~~~

加入master集群

~~~shell
kubeadm join 192.168.25.10:6443 --token xg7ibt.8qonzrnjziedn8o6 --discovery-token-ca-cert-hash sha256:bba7097fa4fb06aab523d24f6788e5552a86202fc7a74ed09406fb21915dd87c
~~~





所有节点记得将以下服务设置开机启动

~~~shell
systemctl enable docker
systemctl enable cri-docker
systemctl enable kubelet
~~~





master看到一下则为成功搭建集群

~~~shell
kubectl get nodes

NAME       STATUS     ROLES           AGE     VERSION
server10   NotReady   control-plane   8m29s   v1.26.3
server11   NotReady   <none>          5m20s   v1.26.3
server12   NotReady   <none>          5m3s    v1.26.3
~~~



### Flannel网络组件安装

~~~shell
cd /etc/kubernetes/manifests
wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
~~~

查看组件所需镜像

~~~shell
grep  "image:"  kube-flannel.yml 

image: docker.io/flannel/flannel:v0.23.0
image: docker.io/flannel/flannel-cni-plugin:v1.2.0
image: docker.io/flannel/flannel:v0.23.0
~~~

修改配置文件

~~~shell
vim /etc/kubernetes/manifests/kube-flannel.yml
#约91行，修改"Network",修改成集群初始化的配置 --pod-network-cidr 指定的ip
 89   net-conf.json: |
 90     {
 91       "Network": "10.2.0.0/16",
 92       "Backend": {
 93         "Type": "vxlan"
 94       }
 95     }
 
#约138行，修改"args"里的数组，添加上--iface=ens33
135       containers:
136       - args:
137         - --ip-masq
138         - --kube-subnet-mgr
139         - --iface=ens33

~~~

启动组件

~~~shell
kubectl apply -f kube-flannel.yml
~~~

启动之后成功，将NotReady改为Ready

~~~shell
kubectl get nodes

NAME       STATUS   ROLES           AGE   VERSION
server10   Ready    control-plane   14m   v1.26.3
server11   Ready    <none>          11m   v1.26.3
server12   Ready    <none>          11m   v1.26.3
~~~

### 升级内核

centos7系统内核 >=4.1 (3.10的内核运行高版本的k8s集群存在不稳定因素)

内核的yum源

~~~shell
cat > /etc/yum.repos.d/kernel.repo << EOF
[kernel]
name="kernel install."
baseurl=https://elrepo.org/linux/kernel/el7/x86_64/
enabled=1
gpgcheck=0
EOF
~~~

安装新内核

~~~shell
yum install kernel-lt-5.4.260
#或者载rpm包本地安装
# rpm -ivh /tmp/kernel-lt-5.4.260-1.el7.elrepo.x86_64.rpm
~~~

设置开机默认启动使用新内核

~~~shell
grub2-set-default 5.4.260-1.el7.elrepo.x86_64
~~~

重启

~~~shell
reboot
~~~

查看新内核

~~~shell
uname -r
~~~



### 修改kube-proxy使用ipvs

使用edit修改

~~~shell
kubectl edit cm -n kube-system kube-proxy

#约48行
#把mode: "" 改成 mode: "ipvs"
#因为edit属于热更改，等一段时间也会自己更新为ipvs模式
~~~

删除原来的kube-proxy

~~~shell
kubectl delete pod -l  k8s-app=kube-proxy -n kube-system
~~~

通过查看kube-proxy的日志可以看到使用信息

~~~shell
kubectl logs -n kube-system kube-proxy-pzxr9

#有类似此类信息  "Using ipvs Proxier"
~~~

通过ipvsadm可以查看到代理规则

~~~shell
ipvsadm -Ln
~~~





## Kubernetes多master高可用集群搭建

### 环境

硬件环境：Centos7，2C2G

| hosts            | ip            |
| ---------------- | ------------- |
| server10(master) | 192.168.25.10 |
| server11(master) | 192.168.25.11 |
| server12(master) | 192.168.25.12 |
| server13         | 192.168.25.13 |



### 系统整体环境配置，所有节点又要配置

~~~shell
#语言设置
localectl set-locale LANG=en_US.utf8
#epel源
wget -O /etc/yum.repos.d/epel.repo https://mirrors.aliyun.com/repo/epel-7.repo
#设置时区
timedatectl set-timezone Asia/Shanghai
#校正系统时间
yum install -y ntpdate
ntpdate ntp.aliyun.com
#将时间写入本地硬件时钟 RTC
timedatectl set-local-rtc 0
#清除及关闭firewalld,关闭selinux
iptables -F
systemctl  stop firewalld
systemctl disable firewalld
setenforce 0
sed -i '/SELINUX=/c\SELINUX=disabled/' /etc/selinux/config
#关闭swap（因为swap分区使用会影响k8s性能）
swapoff -a
sed -i '/swap/s/.*/#&/' /etc/fstab
#设置内核及相关网络参数
yum install -y ipvsadm ipset
modprobe br_netfilter
modprobe overlay
modprobe ip_vs_sh
modprobe ip_vs_wrr
modprobe ip_vs_rr
modprobe ip_vs

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
ip_vs_sh
ip_vs_wrr
ip_vs_rr
ip_vs
EOF

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
net.ipv4.tcp_tw_recycle=0
vm.swappiness=0
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_instances=8192
fs.inotify.max_user_watches=1048576
fs.file-max=52706963
fs.nr_open=52706963
net.ipv6.conf.all.disable_ipv6=1
net.netfilter.nf_conntrack_max=2310720
EOF

sysctl -p /etc/sysctl.d/k8s.conf
~~~

配置定时任务

~~~shell
crontab  -e

01 */2 * * *  ntpdata   npt.aliyun.com
~~~

~~~shell
systemctl  restart crond.service
~~~

配置A记录解析

~~~shell
cat > /etc/hosts << EOF
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

192.168.25.10  server10
192.168.25.11  server11
192.168.25.12  server12
192.168.25.13  server13
192.168.25.20  server20  hub.registry.com
EOF
~~~

安装docer-cd

官方文档：https://docs.docker.com/engine/install/centos/

~~~shell
#卸载机器上老版本
yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-engine
#配置yum源
yum  install -y yum-utils
yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo
#安装
yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
#docker镜像加速及私有仓库信任
cat > /etc/docker/daemon.json << EOF
{
   "registry-mirrors":["https://hub-mirror.c.163.com/"],
   "insecure-registries":["http://hub.registry.com"]
}
EOF
#启动
systemctl start docker
systemctl enable docker
~~~

配置kubernetes 阿里仓库源

~~~shell
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
~~~

~~~shell
yum clean all
yum makecache fast
~~~

安装cri-dockerd

地址：https://github.com/Mirantis/cri-dockerd

~~~shell
yum install -y go git
~~~

~~~shell
tar -xf /tmp/cri-dockerd.tar.gz
~~~

~~~shell
cd cri-dockerd

make cri-dockerd

install -o root -g root -m 0755 cri-dockerd    /usr/bin/cri-dockerd

install packaging/systemd/* /usr/lib/systemd/system/
~~~

配置cri-dockerd的systemd文件

~~~shell
cat > /usr/lib/systemd/system/cri-docker.service << EOF               
[Unit]
Description=CRI Interface for Docker Application Container Engine
Documentation=https://docs.mirantis.com
After=network-online.target firewalld.service docker.service
Wants=network-online.target
Requires=cri-docker.socket
[Service]
Type=notify
ExecStart=/usr/bin/cri-dockerd --container-runtime-endpoint fd:// --network-plugin=cni --cni-bin-dir=/opt/cni/bin --cni-cache-dir=/var/lib/cni/cache --cni-conf-dir=/etc/cni/net.d --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9
ExecReload=/bin/kill -s HUP 
TimeoutSec=0
RestartSec=2
Restart=always
StartLimitBurst=3
StartLimitInterval=60s
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
TasksMax=infinity
Delegate=yes
KillMode=process
[Install]
WantedBy=multi-user.target
EOF
~~~

配置cri-dockerd的socket文件

~~~shell
cat > /usr/lib/systemd/system/cri-docker.socket  << EOF 
[Unit]
Description=CRI Docker Socket for the API
PartOf=cri-docker.service

[Socket]
ListenStream=%t/cri-dockerd.sock
SocketMode=0660
SocketUser=root
SocketGroup=root

[Install]
WantedBy=sockets.target
EOF
~~~

~~~shell
systemctl daemon-reload
systemctl start cri-docker.service
systemctl enable cri-docker.service
~~~

安装kubeadm工具

~~~shell
#查看kubernetes历史版本
yum list --showduplicates | grep kube
~~~

~~~shell
#安装对应版本
yum install -y kubelet-1.26.3 kubectl-1.26.3 kubeadm-1.26.3
~~~

~~~shell
systemctl enable kubelet
~~~



### 部署apiServer的负载均衡器

所有master节点都操作

~~~shell
yum install -y haproxy keepalived
~~~

haproxy配置文件

~~~shell
vim /etc/haproxy/haproxy.cfg

#三个master的haproxy配置文件可以相同 
global
    log         127.0.0.1 local2
    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon

    stats socket /var/lib/haproxy/stats

defaults
    mode                    tcp
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000

frontend    k8s-api
   #haproxy的工作端口
   bind     :16443
   mode     tcp
   maxconn  50000
   default_backend  k8s-api-backend

backend  k8s-api-backend
    #三个master进行负载均衡
    balance roundrobin
    server  server10  192.168.25.10:6443  weight 1 check inter 5s rise 2 fall 3
    server  server11  192.168.25.11:6443  weight 1 check inter 5s rise 2 fall 3
    server  server13  192.168.25.12:6443  weight 1 check inter 5s rise 2 fall 3
~~~

keepalived配置文件

~~~shell
vim /etc/keepalived/keepalived.conf 

! Configuration File for keepalived
global_defs {
   #每个master的router_id 要不相同
   router_id  server10
   vrrp_mcast_group4  224.3.3.3
   script_user  root
   enable_script_security
}

vrrp_script chklive {
    script  /etc/keepalived/chkhaproxy.sh
    interval 1
    weight  -3
    timeout  5
}

vrrp_instance VI_1 {
    nopreempt
    state     BACKUP
    interface ens33
    #可以根据情况修改优先级
    priority  100
    virtual_router_id 51

    advert_int 3
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        #改成自己的VIP
        192.168.25.99
    }

    track_script {
        chklive
    }
}
~~~

keepalived脚本

~~~shell
vim /etc/keepalived/chkhaproxy.sh 

#! /bin/bash

if ! netstat  -nlpt | grep -w "16443" &> /dev/null
then
    systemctl  stop keepalived
fi
exit 0
~~~

给脚本权限

~~~shell
chmod +x /etc/keepalived/chkhaproxy.sh
~~~

设置开机自启并启动

~~~shell
systemctl enable --now haproxy
systemctl enable --now keepalived
~~~

使用**ip addr**查看VIP在哪台master，然后对有VIP的master测试VIP是否会漂移

~~~shell
systemctl stop haproxy
~~~

VIP会根据keepalived设置的优先级进行漂移，查看其他master节点，使用ip addr查看VIP是否已经漂移到别的节点，若出现VIP则高可用成功

测试完后要记得把haproxy和keepalived重启启动。



### 初始化集群

在有VIP的master节点上执行命令

~~~shell
kubeadm init --apiserver-advertise-address=192.168.25.10 --kubernetes-version=v1.26.3 --image-repository registry.aliyuncs.com/google_containers --cri-socket unix:///var/run/cri-dockerd.sock --service-cidr=10.1.0.0/16 --pod-network-cidr=10.2.0.0/16 --control-plane-endpoint "192.168.25.99:16443" --upload-certs 
~~~

成功则显示这个

~~~shell
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 192.168.25.99:16443 --token 0ojcff.13901wo8w8u60f92 \
	--discovery-token-ca-cert-hash sha256:81b112b4737542d2410c99a851c1917070ce5a291fa120145d5289782e4c7ff6 \
	--control-plane --certificate-key e3b7091a454d1b1cf9f5ca19bfbd94749ab306c03192118088f6866430e3a132

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary,you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.25.99:16443 --token 0ojcff.13901wo8w8u60f92 \
	--discovery-token-ca-cert-hash sha256:81b112b4737542d2410c99a851c1917070ce5a291fa120145d5289782e4c7ff6
~~~

其他master节点加入集群，这里的token和证书有有效期，有效期过了token就会失效，想要再加入集群需要再次创建token，使用命令**`kubeadm token create`** 创建token

~~~~shell
kubeadm join 192.168.25.99:16443 --token 0ojcff.13901wo8w8u60f92 \
	--discovery-token-ca-cert-hash sha256:81b112b4737542d2410c99a851c1917070ce5a291fa120145d5289782e4c7ff6 \
	--control-plane --certificate-key e3b7091a454d1b1cf9f5ca19bfbd94749ab306c03192118088f6866430e3a132 \
	--cri-socket unix:///var/run/cri-dockerd.sock
~~~~

成功后根据提示

~~~shell
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
~~~

加入node节点

~~~shell
kubeadm join 192.168.25.99:16443 --token 0ojcff.13901wo8w8u60f92 \
	--discovery-token-ca-cert-hash sha256:81b112b4737542d2410c99a851c1917070ce5a291fa120145d5289782e4c7ff6 \
	--cri-socket unix:///var/run/cri-dockerd.sock
~~~

查看集群状态

~~~shell
kubectl get nodes

NAME       STATUS   ROLES           AGE   VERSION
server10   Ready    control-plane   8d    v1.26.3
server11   Ready    control-plane   8d    v1.26.3
server12   Ready    control-plane   8d    v1.26.3
server13   Ready    <none>          8d    v1.26.3
~~~



### 配置kubernetes命令shell  tab补全

官方文档：https://v1-26.docs.kubernetes.io/zh-cn/docs/tasks/tools/install-kubectl-linux/

~~~shell
yum install -y bash-com*
kubectl completion bash | sudo tee /etc/bash_completion.d/kubectl > /dev/null
chmod a+r /etc/bash_completion.d/kubectl
echo 'alias k=kubectl' >>~/.bashrc
echo 'complete -o default -F __start_kubectl k' >>~/.bashrc
~~~

~~~shell
 #最后
 exit
~~~



### Flannel网络组件安装

只在master节点上安装

~~~shell
cd /etc/kubernetes/manifests
wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
~~~

查看组件所需镜像

~~~shell
grep  "image:"  kube-flannel.yml 

image: docker.io/flannel/flannel:v0.23.0
image: docker.io/flannel/flannel-cni-plugin:v1.2.0
image: docker.io/flannel/flannel:v0.23.0
~~~

修改配置文件

~~~shell
vim /etc/kubernetes/manifests/kube-flannel.yml
#约91行，修改"Network",修改成集群初始化的配置 --pod-network-cidr 指定的ip
 89   net-conf.json: |
 90     {
 91       "Network": "10.2.0.0/16",
 92       "Backend": {
 93         "Type": "vxlan"
 94       }
 95     }
 
#约138行，修改"args"里的数组，添加上--iface=ens33
135       containers:
136       - args:
137         - --ip-masq
138         - --kube-subnet-mgr
139         - --iface=ens33

~~~

启动组件

~~~shell
kubectl apply -f kube-flannel.yml
~~~

启动之后成功，在所有master节点上运行，NotReady变为Ready

~~~shell
kubectl get nodes

NAME       STATUS   ROLES           AGE   VERSION
server10   Ready    control-plane   14m   v1.26.3
server11   Ready    <none>          11m   v1.26.3
server12   Ready    <none>          11m   v1.26.3
~~~



### 升级内核

centos7系统内核 >=4.1 (3.10的内核运行高版本的k8s集群存在不稳定因素)

内核的yum源

~~~shell
cat > /etc/yum.repos.d/kernel.repo << EOF
[kernel]
name="kernel install."
baseurl=https://elrepo.org/linux/kernel/el7/x86_64/
enabled=1
gpgcheck=0
EOF
~~~

安装新内核

~~~shell
yum install kernel-lt-5.4.260
#或者载rpm包本地安装
# rpm -ivh /tmp/kernel-lt-5.4.260-1.el7.elrepo.x86_64.rpm
~~~

设置开机默认启动使用新内核

~~~shell
grub2-set-default 5.4.260-1.el7.elrepo.x86_64
~~~

重启

~~~shell
reboot
~~~

查看新内核

~~~shell
uname -r
~~~



### 修改kube-proxy使用ipvs

使用edit修改

~~~shell
kubectl edit cm -n kube-system kube-proxy

#约48行
#把mode: "" 改成 mode: "ipvs"
#因为edit属于热更改，等一段时间也会自己更新为ipvs模式
~~~

删除原来的kube-proxy

~~~shell
kubectl delete pod -l  k8s-app=kube-proxy -n kube-system
~~~

通过查看kube-proxy的日志可以看到使用信息

~~~shell
kubectl logs -n kube-system kube-proxy-pzxr9

#有类似此类信息  "Using ipvs Proxier"
~~~

通过ipvsadm可以查看到代理规则

~~~shell
ipvsadm -Ln
~~~

