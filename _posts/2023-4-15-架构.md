---
layout: post
title: LNMT架构的Web应用项目
subtitle: 对所学知识做个总结
categories: Linux
author: FatGuy010
permalink: /2023/04/15/LNMTLargeArchitecture
tags: [Linux]
---



### 架构图

![](\assets\images\LNMT.png)

### 环境说明

~~~shell
# cat /etc/centos-release
CentOS Linux release 7.9.2009 (Core)

# cat /etc/os-release
NAME="CentOS Linux"
VERSION="7 (Core)"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="7"
PRETTY_NAME="CentOS Linux 7 (Core)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:centos:centos:7"
HOME_URL="https://www.centos.org/"
BUG_REPORT_URL="https://bugs.centos.org/"

CENTOS_MANTISBT_PROJECT="CentOS-7"
CENTOS_MANTISBT_PROJECT_VERSION="7"
REDHAT_SUPPORT_PRODUCT="centos"
REDHAT_SUPPORT_PRODUCT_VERSION="7"

# uname -a
Linux nginx134 3.10.0-1160.el7.x86_64 #1 SMP Mon Oct 19 16:18:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
~~~

|                 hostname                 |                   ip                    |
| :--------------------------------------: | :-------------------------------------: |
|        gitlab130        (gitlab)         |             192.168.25.130              |
|     jenkins131     (jenkins+ansible)     |             192.168.25.131              |
|       nginxDR132   (ipvsadm+nginx)       | 192.168.25.132 --->  VIP:192.168.25.100 |
|         nginx133         (nginx)         | 192.168.25.133 --->  VIP:192.168.25.100 |
|        tomcat134         (tomcat)        |             192.168.25.134              |
|         tomcat135       (tomcat)         |             192.168.25.135              |
|         tomcat136       (tomcat)         |             192.168.25.136              |
|      mycatdr137    (ipvsamd+mycat)       | 192.168.25.137 --->  VIP:192.168.25.106 |
|         mycat138         (mycat)         |             192.168.25.138              |
|    mysql139          (mysql)(master)     |             192.168.25.139              |
|    mysql140          (mysql)(slave1)     |             192.168.25.140              |
|    mysql141  (mysql)(slave2+manager)     | 192.168.25.141  ---> VIP:192.168.25.199 |
|                zabbix142                 |             192.168.25.142              |
|                 redis143                 |             192.168.25.143              |
|               logstash144                |             192.168.25.144              |
| elasticsearch145  (elasticsearch+kibana) |             192.168.25.145              |

### A记录解析

~~~shell
cat > /etc/hosts << EOF
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

192.168.25.130 gitlab130
192.168.25.131 jenkins131
192.168.25.132 nginxdr132
192.168.25.133 nginx133
192.168.25.134 tomcat134
192.168.25.135 tomcat135
192.168.25.136 tomcat136
192.168.25.137 mycatdr137
192.168.25.138 mycat138
192.168.25.139 mysql139
192.168.25.140 mysql140
192.168.25.141 mysql141
192.168.25.142 zabbix142
192.168.25.143 redis143
192.168.25.144 logstash144
192.168.25.145 elasticsearch145
EOF
~~~

### gitlab安装部署

配置依赖环境

```shell
yum install -y curl policycoreutils-python openssh-server openssh-clirnts postfixcronie lokkit rpm
```

安装gitlab

```shell
rpm -ivh /tmp/gitlab-ce-16.0.0-ce.0.el7.x86_64.rpm
```

修改配置文件

~~~shell
vim /etc/gitlab/gitlab.rb 

external_url 'http://192.168.25.130'
user['username'] = "git"
user['group'] = "git"
~~~

修改配置文件后

~~~shell
gitlab-ctl reconfigure
~~~

成功后

~~~shell
Notes:
Default admin account has been configured with following details:
Username: root
Password: You didn't opt-in to print initial root password to STDOUT.
Password stored to /etc/gitlab/initial_root_password. This file will be cleaned up in first reconfigure run after 24 hours.

NOTE: Because these credentials might be present in your log files in plain text, it is highly recommended to reset the password following https://docs.gitlab.com/ee/security/reset_user_password.html#reset-your-root-password.

gitlab Reconfigured!
[root@gitlab130 tmp]#
~~~

查看初始密码

~~~shell
cat /etc/gitlab/initial_root_password

# WARNING: This value is valid only in the following conditions
#          1. If provided manually (either via `GITLAB_ROOT_PASSWORD` environment variable or via `gitlab_rails['initial_root_password']` setting in `gitlab.rb`, it was provided before database was seeded for the first time (usually, the first reconfigure run).
#          2. Password hasn't been changed manually, either via UI or via command line.
#
#          If the password shown here doesn't work, you must reset the admin password following https://docs.gitlab.com/ee/security/reset_user_password.html#reset-your-root-password.

Password: 3+ZV2iUGZGUhtVXNvt40wD3z6Fwur/RSgBKGxZ/w6vk=

# NOTE: This file will be automatically deleted in the first reconfigure run after 24 hours.

~~~

### jenkins安装部署

安装jdk11

~~~shell
tar -xf /tmp/jdk-11.0.18_linux-x64_bin.tar.gz -C /usr/local/
~~~

~~~shell
mv /usr/local/jdk-11.0.18/  /usr/local/jdk-11
~~~

~~~shell
yum install -y dejavu-sans-fonts fontconfig xorg-x11-server-Xvfb
~~~

配置jdk环境变量

~~~shell
vim  /etc/profile.d/jdk.sh

#! /bin/bash
JAVA_HOME=/usr/local/jdk-11
JRE_HOME=$JAVA_HOME/jre
PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin
CLASSPATH=$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib
export JAVA_HOME JRE_HOME CLASSPATH
~~~

~~~shell
./etc/profile
~~~

~~~shell
 ln -s /usr/local/jdk-11/bin/java   /usr/bin/
~~~

安装jenkins

~~~shell
yum localinstall -y  /tmp/jenkins-2.361.2-1.1.noarch.rpm
~~~

~~~shell
systemctl  daemon-reload 
~~~

配置jenkins

~~~shell
vim  /etc/init.d/jenkins

把  /usr/bin/java  改成
/usr/local/jdk-11/bin/java
~~~

~~~shell
vim /etc/sysconfig/jenkins

把  JENKINS_JAVA_CMD="" 改成
JENKINS_JAVA_CMD="/usr/local/jdk-11/bin/java"
~~~

~~~shell
systemctl  daemon-reload 
~~~

~~~shell
systemctl start jenkins
~~~

查看jenkins初始密码

~~~shell
cat /var/lib/jenkins/secrets/initialAdminPassword
bab9d0fc39994a43ae30a5570b293832
~~~

进去后在最下面的 jenkins中文社区修改镜像

升级站点的url填进 清华镜像

~~~
https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json
~~~

### nginx-DR

安装ipvsamd命令

~~~shell
yum install -y ipvsadm
~~~

配置ens33:0 (VIP)

~~~shell
cp  /etc/sysconfig/network-scripts/ifcfg-ens33  /etc/sysconfig/network-scripts/ifcfg-ens33:0
~~~

~~~shell
vim  /etc/sysconfig/network-scripts/ifcfg-ens33:0

TYPE=Ethernet
BOOTPROTO=static
DEFROUTE=yes
NAME=ens33:0
DEVICE=ens33:0
ONBOOT=yes
IPADDR=192.168.25.100
PREFIX=32
GATEWAY=192.168.25.2
DNS1=192.168.25.2
DNS2=8.8.8.8
~~~

~~~shell
systemctl restart network
~~~

配置LVS-DR规则

~~~shell
ipvsadm -C  #清空所有规则
ipvsadm -A -t 192.168.25.100:80 -s rr  #定义LVS服务，并指定负责均衡策略为rr，即轮询
ipvsadm -a -t 192.168.25.100:80 -r 192.168.25.132:80 -g  #添加一台后端web服务器，作为负载均衡节点，并指定为DR模式
ipvsadm -a -t 192.168.25.100:80 -r 192.168.25.133:80 -g  #添加一台后端web服务器，作为负载均衡节点，并指定为DR模式
ipvsadm  --save >  /etc/sysconfig/ipvsadm  #保存负载均衡策略规则
systemctl restart ipvsadm.service
ipvsadm -Ln
~~~

~~~shell
route add -host 192.168.25.100 dev ens33:0
~~~

查看LVS-DR

~~~shell
ipvsadm -Ln --stats
~~~

~~~shell
ipvsadm -Ln --rate
~~~

### nginx代理配置

配置nginx源

~~~shell
vim /etc/yum.repos.d/nginx.repo

[nginx-stable]
name=nginx stable repo
baseurl=http://nginx.org/packages/centos/$releasever/$basearch/
gpgcheck=0
enabled=1
gpgkey=https://nginx.org/keys/nginx_signing.key
module_hotfixes=true
~~~

安装nginx

~~~shell
yum install -y nginx
~~~

配置LVS-DR

~~~shell
cp /etc/sysconfig/network-scripts/ifcfg-lo /etc/sysconfig/network-scripts/ifcfg-lo:0
~~~

~~~shell
vim /etc/sysconfig/network-scripts/ifcfg-lo:0

DEVICE=lo:0
IPADDR=192.168.25.100
NETMASK=255.255.255.255
ONBOOT=yes
NAME=lo:0
~~~

~~~shell
systemctl restart network
~~~

优化

~~~shell
vim /etc/sysctl.conf 
 
net.ipv4.conf.lo.arp_ignore=1
net.ipv4.conf.ens33.arp_ignore=1
net.ipv4.conf.all.arp_ignore=1
net.ipv4.conf.all.arp_announce=2
net.ipv4.conf.ens33.arp_announce=2
net.ipv4.conf.lo.arp_announce=2
~~~

~~~shell
sysctl -p
~~~

添加路由

~~~shell
route add -host 192.168.25.100 dev lo:0
~~~

nginx主配值文件

~~~shell
vim /etc/nginx/nginx.conf

user  nginx;
worker_processes  auto;
error_log  /var/log/nginx/error.log notice;
pid        /var/run/nginx.pid;

events {
    use epoll;
    worker_connections  1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

#    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
#                      '$status $body_bytes_sent "$http_referer" '
#                      '"$http_user_agent" "$http_x_forwarded_for"';

   log_format  main   '{"remote_addr":"$remote_addr","remote_user":"$remote_user",'
                       '"time_local":"$time_local","request":"$request_method","uri":"$uri","status":"$status",'
                      '"body_size":"$body_bytes_sent","referer":"$http_referer","agent":"$http_user_agent",'
                       '"x_forwarded":"$http_x_forwarded_for"}';

    access_log  /var/log/nginx/access.log  main;
    sendfile        on;
    tcp_nopush     on;
    keepalive_timeout  65;
    gzip  on;
    include /etc/nginx/conf.d/*.conf;

    upstream tomcat {
        server 192.168.25.134:8080 weight=1  max_fails=3  fail_timeout=10;
        server 192.168.25.135:8080 weight=1  max_fails=3  fail_timeout=10;
        server 192.168.25.136:8080 weight=1  max_fails=3  fail_timeout=10;
    }
}
~~~

nginx子配置文件

~~~shell
vim /etc/nginx/conf.d/default.conf

roxy_cache_path /etc/nginx/zrlog_cache levels=1:2 keys_zone=zrlog_cache:100m max_size=10g inactive=1d use_temp_path=off;
proxy_headers_hash_max_size  2048;
proxy_headers_hash_bucket_size  512;
server {
    listen       80;
    server_name  localhost;
    access_log  /var/log/nginx/tomcat.access.log  main;
    charset utf-8;
    #proxy_bind  192.168.25.100;
    proxy_buffering on;
    proxy_buffer_size  128k;
    proxy_buffers  32  16k;
    proxy_send_timeout  30s;
    proxy_read_timeout  30s;
    proxy_connect_timeout  5s;

    location / {
        proxy_pass   http://tomcat;
        proxy_set_header Host    $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
    
    location = /ngx_status {
        stub_status;
        allow 127.0.0.1;
        deny all;
    }
}
~~~

启动nginx

~~~shell
systemctl restart nginx
systemctl enable --now nginx
~~~

### tomcat集群

配置jdk环境

~~~shell
tar xf /tmp/jdk-8u201-linux-x64.tar.gz -C /usr/local/
~~~

~~~shell
mv /usr/local/jdk1.8.0_201 /usr/local/jdk1.8
~~~

~~~shell
vim /etc/profile.d/jdk.sh

JAVA_HOME=/usr/local/jdk1.8
JRE_HOME=$JAVA_HOME/jre
CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib
PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin
export JAVA_HOME JRE_HOME CLASSPATH
~~~

~~~shell
. /etc/profile
~~~

部署tomcat

~~~shell
tar -xf /tmp/apache-tomcat-8.5.93.tar.gz -C /opt/
~~~

~~~shell
mv /opt/apache-tomcat-8.5.93 /opt/tomcat-8.5
~~~

~~~shell
vim /usr/bin/tomcat

#! /bin/bash

if [ $# -ne 1 ]
then
    echo "use error.  use : tomcat  start/stop/restart" >&2;
    exit 1
fi

case $1 in
    start)
        /opt/tomcat-8.5/bin/startup.sh
        ;;
    stop)
        /opt/tomcat-8.5/bin/shutdown.sh 
        ;;
    restart)
        /opt/tomcat-8.5/bin/shutdown.sh
        sleep  0.2
        /opt/tomcat-8.5/bin/startup.sh
        ;;
    *)
        echo   "tomcat  start/stop/restart"
        exit 2
        ;;
esac

exit 0
~~~

~~~shell
chmod a+x /usr/bin/tomcat
~~~

修改tomcat配置文件

~~~xml
vim /opt/tomcat-8.5/conf/server.xml

<!--约173行，把下面这段注释起来-->
<Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs"
175                prefix="localhost_access_log" suffix=".txt"
176                pattern="%h %l %u %t &quot;%r&quot; %s %b" />

<!--使用下面这段，这可以使tomcat生成的日志为json格式-->
<Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs"
                prefix="localhost_access_log" suffix=".txt"
                pattern="{&quot;clientip&quot;:&quot;%h&quot;,&quot;ClientUser&quot;:&quot;%l&quot;,&quot;authenticated&quot;:&quot;%u&quot;,&quot;AccessTime&quot;:&quo    t;%t&quot;,&quot;method&quot;:&quot;%r&quot;,&quot;status&quot;:&quot;%s&quot;,&quot;SendBytes&quot;:&quot;%b&quot;,&quot;Query?string&quot;:&quot;%q&quot;,&quot;partn    er&quot;:&quot;%{Referer}i&quot;,&quot;AgentVersion&quot;:&quot;%{User-Agent}i&quot;}" />
~~~

启动tomcat

~~~
tomcat start
~~~

### mycat-DR

安装ipvsamd命令

~~~shell
yum install -y ipvsadm
~~~

配置ens33:0 (VIP)

~~~shell
cp  /etc/sysconfig/network-scripts/ifcfg-ens33  /etc/sysconfig/network-scripts/ifcfg-ens33:0
~~~

~~~shell
vim  /etc/sysconfig/network-scripts/ifcfg-ens33:0

TYPE=Ethernet
BOOTPROTO=static
DEFROUTE=yes
NAME=ens33:0
DEVICE=ens33:0
ONBOOT=yes
IPADDR=192.168.25.106
PREFIX=32
GATEWAY=192.168.25.2
DNS1=192.168.25.2
DNS2=8.8.8.8
~~~

~~~shell
systemctl restart network
~~~

配置LVS-DR规则

~~~shell
ipvsadm -C  #清空所有规则
ipvsadm -A -t 192.168.25.106:8066 -s rr  #定义LVS服务，并指定负责均衡策略为rr，即轮询
ipvsadm -a -t 192.168.25.106:8066 -r 192.168.25.137:8066 -g  #添加一台后端web服务器，作为负载均衡节点，并指定为DR模式
ipvsadm -a -t 192.168.25.106:8066 -r 192.168.25.138:8066 -g  #添加一台后端web服务器，作为负载均衡节点，并指定为DR模式
ipvsadm  --save >  /etc/sysconfig/ipvsadm  #保存负载均衡策略规则
systemctl restart ipvsadm.service
route add -host 192.168.25.106:8066 dev ens33:0
ipvsadm -Ln
~~~

查看LVS-DR

~~~shell
ipvsadm -Ln --stats
~~~

~~~shell
ipvsadm -Ln --rate
~~~

### mycat

配置jdk环境

~~~shell
tar xf /tmp/jdk-8u201-linux-x64.tar.gz -C /usr/local/
~~~

~~~shell
mv /usr/local/jdk1.8.0_201 /usr/local/jdk1.8
~~~

~~~shell
vim /etc/profile.d/jdk.sh

JAVA_HOME=/usr/local/jdk1.8
JRE_HOME=$JAVA_HOME/jre
CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib
PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin
export JAVA_HOME JRE_HOME CLASSPATH
~~~

~~~shell
. /etc/profile
~~~

部署mycat

~~~shell
tar xf /tmp/Mycat-server-1.6.5-release-20180122220033-linux.tar.gz -C /usr/local/
~~~

添加mycat用户

~~~shell
groupadd mycat
~~~

~~~shell
useradd -g mycat mycat
~~~

~~~shell
echo "123456" | passwd --stdin mycat
~~~

配置systemctl启动服务

~~~shell
chown -Rf mycat:mycat /usr/local/mycat
~~~

~~~shell
vim /usr/lib/systemd/system/mycat.service

[Unit]
Description=mycat Database Proxy
Description=mycat
After=syslog.target
After=network.target

[Service]
Type=simple
Restart=on-abort
PIDFile=/usr/local/mycat/logs/mycat.pid
ExecStart=/usr/local/mycat/bin/mycat start
PrivateTmp=true

[Install]
WantedBy=multi-user.target
~~~

#### schema.xml

~~~xml
<?xml version="1.0"?>
<!DOCTYPE mycat:schema SYSTEM "schema.dtd">
<mycat:schema xmlns:mycat="http://io.mycat/">

        <schema name="TESTDB1" checkSQLschema="false" sqlMaxLimit="100" dataNode="dn139"></schema>
        <dataNode name="dn139" dataHost="192.168.25.139" database="testbookdb" />
        <dataHost name="192.168.25.139" maxCon="1000" minCon="10" balance="3"
                          writeType="0" dbType="mysql" dbDriver="native" switchType="1"  slaveThreshold="100">
                <heartbeat>select user()</heartbeat>
                <writeHost host="192.168.25.139" url="192.168.25.139:3306" user="root"
                                   password="Ckc_123123">
                        <readHost host="192.168.25.140" url="192.168.25.140:3306" user="root" password="Ckc_123123" />
                        <readHost host="192.168.25.141" url="192.168.25.141:3306" user="root" password="Ckc_123123" />
                </writeHost>
        </dataHost>
</mycat:schema>
~~~

#### server.xml

~~~xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE mycat:server SYSTEM "server.dtd">
<mycat:server xmlns:mycat="http://io.mycat/">
	<system>
	<property name="nonePasswordLogin">0</property>
	<property name="useHandshakeV10">1</property>
	<property name="useSqlStat">0</property>  
	<property name="useGlobleTableCheck">0</property>
		<property name="sequnceHandlerType">2</property>
	<property name="subqueryRelationshipCheck">false</property> 
		<property name="processorBufferPoolType">0</property>
		<property name="handleDistributedTransactions">0</property>
		<property name="useOffHeapForMerge">1</property>
        <property name="memoryPageSize">64k</property>
		<property name="spillsFileBufferSize">1k</property>
		<property name="useStreamOutput">0</property>
		<property name="systemReserveMemorySize">384m</property>
		<property name="useZKSwitch">false</property>
	</system>
	<user name="root" defaultAccount="true">
		<property name="password">Ckc_123123</property>
		<property name="schemas">TESTDB1</property>
	</user>
</mycat:server>
~~~

#### log4j2.xml

~~~xml
<?xml version="1.0" encoding="UTF-8"?>
<Configuration status="WARN">
    <Appenders>
        <Console name="Console" target="SYSTEM_OUT">
            <PatternLayout pattern="%d [%-5p][%t] %m %throwable{full} (%C:%F:%L) %n"/>
        </Console>

        <RollingFile name="RollingFile" fileName="${sys:MYCAT_HOME}/logs/mycat.log"
                     filePattern="${sys:MYCAT_HOME}/logs/$${date:yyyy-MM}/mycat-%d{MM-dd}-%i.log.gz">
        <PatternLayout>
                <Pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %5p [%t] (%l) - %m%n</Pattern>
            </PatternLayout>
            <Policies>
                <OnStartupTriggeringPolicy/>
                <SizeBasedTriggeringPolicy size="250 MB"/>
                <TimeBasedTriggeringPolicy/>
            </Policies>
        </RollingFile>
    </Appenders>
    <Loggers>
        <asyncRoot level="debug" includeLocation="true">
            <AppenderRef ref="RollingFile"/>
        </asyncRoot>
    </Loggers>
</Configuration>
~~~


优化配置

~~~shell
vim /usr/local/mycat/conf/wrapper.conf

wrapper.java.command=java  #修改第五行，改成以下内容

wrapper.java.command=/usr/local/jdk1.8/bin/java
~~~

启动

~~~shell
systemctl daemon-reload
~~~

~~~~shell
systemctl restart mycat
~~~~



### MySQL

#### 安装MySQL

安装MySQL源并改成MySQL5.7

~~~shell
rpm -ivh https://dev.mysql.com/get/mysql80-community-release-el7-11.noarch.rpm
~~~

~~~shell
sed -i '5s/0/1/g' /etc/yum.repos.d/mysql-community.repo
~~~

~~~shell
sed -i '14s/1/0/g' /etc/yum.repos.d/mysql-community.repo
~~~

安装MySQL5.7

~~~shell
yum install -y mysql-community-server mysql-community-libs-compat
~~~

删除MySQL源

~~~shell
rm -rf /etc/yum.repos.d/mysql-community*
~~~

跳过MySQL反向解析

~~~shell
sed -i '$ a skip-name-resolve' /etc/my.cnf
#或者
echo "skip-name-resolve" >> /etc/my.cnf
~~~

跳过MySQL权限表

~~~shell
sed -i '$ a skip-grant-tables' /etc/my.cnf
#或者
echo "skip-grant-tables" >> /etc/my.cnf
~~~

重启MySQL使配置生效

~~~shell
systemctl restart mysqld
~~~

获取MySQL密码

~~~shell
PASSWORD=`grep 'temporary password' /var/log/mysqld.log |awk '{print $11}'`
~~~

修改MySQL用户root 的密码

~~~shell
mysql -uroot -p"`echo $PASSWORD`" --connect-expired-password -e "flush privileges;ALTER USER 'root'@'localhost' identified by 'Ckc_123123';flush privileges;"
~~~

去掉跳过MySQL权限表的配置

~~~shell
sed -i '$d' /etc/my.cnf
~~~

重启MySQL使配置生效

~~~shell
systemctl restart mysqld
~~~

尝试使用修改后的密码登录MySQL

~~~shell
mysql -uroot -pCkc_123123
~~~

#### MySQL搭建主从

修改MySQL的配置文件

master的配置文件添加一下内容

~~~shell
echo "   " >> /etc/my.cnf
echo "log-bin=mysql-bin-master" >> /etc/my.cnf
echo "server-id=1" >> /etc/my.cnf
echo "binlog-ignore-db=mysql" >> /etc/my.cnf
echo "binlog-ignore-db=sys" >> /etc/my.cnf
echo "binlog-ignore-db=information_schema" >> /etc/my.cnf
echo "binlog-ignore-db=performance_schema" >> /etc/my.cnf
echo "lower_case_table_names=1" >> /etc/my.cnf
~~~

slave1

~~~shell
echo "   " >> /etc/my.cnf
echo "server-id=2" >> /etc/my.cnf
echo "lower_case_table_names=1" >> /etc/my.cnf
echo "log-bin=mysql-slave1" >> /etc/my.cnf
echo "binlog-ignore-db=mysql" >> /etc/my.cnf
echo "binlog-ignore-db=sys" >> /etc/my.cnf
echo "binlog-ignore-db=information_schema" >> /etc/my.cnf
echo "binlog-ignore-db=performance_schema" >> /etc/my.cnf
echo "log_slave_updates=1" >> /etc/my.cnf
~~~

slave2

~~~shell
echo "   " >> /etc/my.cnf
echo "server-id=3" >> /etc/my.cnf
echo "lower_case_table_names=1" >> /etc/my.cnf
echo "log-bin=mysql-slave2" >> /etc/my.cnf
echo "binlog-ignore-db=mysql" >> /etc/my.cnf
echo "binlog-ignore-db=sys" >> /etc/my.cnf
echo "binlog-ignore-db=information_schema" >> /etc/my.cnf
echo "binlog-ignore-db=performance_schema" >> /etc/my.cnf
echo "log_slave_updates=1" >> /etc/my.cnf
~~~

重启MySQL

~~~shell
systemctl restart mysqld
~~~





**若出现一下问题则是MySQL的server_uuid有问题，可能是两个从节点的MySQL server_uuid相同导致问题出现**

~~~mysql
Last_IO_Error: Got fatal error 1236 from master when reading data from binary log: 'A slave with the same server_uuid/server_id as this slave has connected to the master; the first event '' at 4, the last event read from './mysql-bin-master.000004' at 154, the last byte read from './mysql-bin-master.000004' at 154.'
~~~

查看MySQL server_uuid

~~~mysql
show global variables like '%uuid';
~~~

若相同则修改server_uuid

~~~shell
vim /var/lib/mysql/auto.cnf
~~~

然后重启MySQL

~~~shell
systemctl restart mysqld
~~~







所有节点授权账号

~~~mysql
#主从账号
grant replication slave on *.* to slave@"%" identified by "Ckc_123123";
#监控账号
grant all privileges on *.* to 'root'@'%' identified  by 'Ckc_123123';
flush privileges;
~~~

查看master状态

~~~mysql
mysql> show master status;
+-------------------------+----------+--------------+-------------------------------------------------+-------------------+
| File                    | Position | Binlog_Do_DB | Binlog_Ignore_DB                                | Executed_Gtid_Set |
+-------------------------+----------+--------------+-------------------------------------------------+-------------------+
| mysql-bin-master.000001 |      590 |              | mysql,sys,information_schema,performance_schema |                   |
+-------------------------+----------+--------------+-------------------------------------------------+-------------------+
1 row in set (0.00 sec)

~~~

slave1

配置主从

~~~mysql
#关闭GTID
change master to master_auto_position=0;
change master to master_host='192.168.25.139',master_user='slave',master_password='Ckc_123123',master_log_file='mysql-bin-master.000001',master_log_pos=590,master_port=3306;
start slave;
show slave status\G
~~~

slave2

配置主从

~~~mysql
#关闭GTID
change master to master_auto_position=0;
change master to master_host='192.168.25.139',master_user='slave',master_password='Ckc_123123',master_log_file='mysql-bin-master.000001',master_log_pos=590,master_port=3306;
start slave;
show slave status\G
~~~

##### 快速建库建表

~~~mysql
CREATE DATABASE `testbookdb` DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;
show databases;
use testbookdb;
CREATE TABLE book1 (bTypeId int,bName char(16),price int,publishing char(16));
INSERT INTO book1(bTypeId,bName,price,publishing) VALUES('1','《Linux从入门到精通》','66','电子工业出版社');
INSERT INTO book1(bTypeId,bName,price,publishing) VALUES('2','《云计算趋势》','68','人民邮电出版社');
INSERT INTO book1(bTypeId,bName,price,publishing) VALUES('3','《操作系统设计与实现》','90','机械工业出版社');
INSERT INTO book1(bTypeId,bName,price,publishing) VALUES('4','《高性能MySQL》','71','清华大学出版社');
INSERT INTO book1(bTypeId,bName,price,publishing) VALUES('5','《高性能MySQL2》','72','清华大学出版社');
INSERT INTO book1(bTypeId,bName,price,publishing) VALUES('6','《高性能MySQL3》','73','清华大学出版社');
INSERT INTO book1(bTypeId,bName,price,publishing) VALUES('7','《高性能MySQL4》','74','清华大学出版社');
INSERT INTO book1(bTypeId,bName,price,publishing) VALUES('8','《高性能MySQL5》','75','清华大学出版社');
INSERT INTO book1(bTypeId,bName,price,publishing) VALUES('9','《高性能MySQL6》','76','清华大学出版社');
INSERT INTO book1(bTypeId,bName,price,publishing) VALUES('10','《高性能MySQL7》','77','清华大学出版社');
INSERT INTO book1(bTypeId,bName,price,publishing) VALUES('11','《高性能MySQL8》','78','清华大学出版社');
INSERT INTO book1(bTypeId,bName,price,publishing) VALUES('12','《高性能MySQL9》','79','清华大学出版社');
 
CREATE TABLE book2(bTypeId int,bName char(16),price int,pages int);
INSERT INTO book2(bTypeId,bName,price,pages) VALUES('1','《Linux从入门到精通》','30','666');
INSERT INTO book2(bTypeId,bName,price,pages) VALUES('2','《云计算趋势》','60','666');
INSERT INTO book2(bTypeId,bName,price,pages) VALUES('3','《操作系统设计与实现》','80','666');
INSERT INTO book2(bTypeId,bName,price,pages) VALUES('4','《高性能MySQL》','166','666');
 
show tables;
select * from book1;
select * from book2;
~~~

#### MySQL搭建MHA高可用

安装依赖（配置好epel源）

~~~shell
yum install -y perl-DBD-MySQL perl-Config-Tiny perl-Log-Dispatch perl-Parallel-ForkManager perl-Time-HiRes perl-ExtUtils-CBuilder perl-ExtUtils-MakeMaker perl-CPAN sshpass
~~~

所有的MySQL节点安装mha-node

~~~shell
rpm -ivh /tmp/mha4mysql-node-0.58-0.el7.centos.noarch.rpm
~~~

manager节点安装mha-manager

~~~shell
rpm -ivh /tmp/mha4mysql-manager-0.58-0.el7.centos.noarch.rpm
~~~

MySQL所有节点配置ssh-key

~~~shell
ssh-keygen -t rsa -N '' -f /root/.ssh/id_rsa -q
~~~

MySQL所有节点配置ssh不需要指纹认证

~~~shell
sed -i '/StrictHostKeyChecking/c StrictHostKeyChecking no' /etc/ssh/ssh_config
~~~

MySQL所有节点配置互信

~~~shell
sshpass -p 123456 ssh-copy-id -i root@192.168.25.139
sshpass -p 123456 ssh-copy-id -i root@192.168.25.140
sshpass -p 123456 ssh-copy-id -i root@192.168.25.141
~~~

两台slave服务器设置read_only（从库对外提供读服务，之所以没有写进配置文件，是因为slave随时会提升为master）

~~~shell
mysql -uroot -pCkc_123123 -e 'set global read_only=1'
~~~

配置MHA manager

~~~shell
mkdir -p /etc/masterha
mkdir -p /var/log/masterha/app1
~~~

mha配置文件

~~~shell
vim /etc/masterha/app1.cnf

[server default]
manager_workdir=/var/log/masterha/app1
manager_log=/var/log/masterha/app1/manager.log
master_binlog_dir=/var/lib/mysql
master_ip_failover_script=/usr/local/bin/master_ip_failover
#master_ip_online_change_script=/usr/local/bin/master_ip_online_change  
password=Ckc_123123
user=root
ping_interval=1
remote_workdir=/tmp
repl_password=Ckc_123123
repl_user=slave
#report_script=/usr/local/send_report
#shutdown_script=""
ssh_user=root

[server1]
hostname=192.168.25.139
port=3306

[server2]
hostname=192.168.25.140
port=3306
candidate_master=1
check_repl_delay=0

[server3]
hostname=192.168.25.141
port=3306
~~~

设置relay log的清除方式（在每个slave节点上）

~~~shell
mysql -uroot -pCkc_123123 -e 'set global relay_log_purge=0'
~~~

在manager结点上检查ssh配置

~~~shell
masterha_check_ssh --conf=/etc/masterha/app1.cnf
~~~

给master配置VIP

~~~shell
ip addr add 192.168.25.199/24 brd 192.168.25.255 dev ens33 label ens33:1
arping -c 1 192.168.25.199
~~~

在manager创建自动切换脚本

~~~shell
vim /usr/local/bin/master_ip_failover

#!/usr/bin/env perl
use strict;
use warnings FATAL => 'all';
 
use Getopt::Long;
 
my (
    $command,          $ssh_user,        $orig_master_host, $orig_master_ip,
    $orig_master_port, $new_master_host, $new_master_ip,    $new_master_port
);
 
my $vip = '192.168.25.199';
my $brdc = '192.168.25.255';
my $ifdev = 'ens33';
my $key = '1';
my $ssh_start_vip = "/usr/sbin/ip addr add $vip/24 brd $brdc dev $ifdev label $ifdev:$key;/usr/sbin/arping -q -A -c 1 -I $ifdev:$key $vip;iptables -F;";
my $ssh_stop_vip = "/usr/sbin/ip addr del $vip/24 dev $ifdev label $ifdev:$key";
 
GetOptions(
    'command=s'          => \$command,
    'ssh_user=s'         => \$ssh_user,
    'orig_master_host=s' => \$orig_master_host,
    'orig_master_ip=s'   => \$orig_master_ip,
    'orig_master_port=i' => \$orig_master_port,
    'new_master_host=s'  => \$new_master_host,
    'new_master_ip=s'    => \$new_master_ip,
    'new_master_port=i'  => \$new_master_port,
);
 
exit &main();
 
sub main {
 
    print "\n\nIN SCRIPT TEST====$ssh_stop_vip==$ssh_start_vip===\n\n";
 
    if ( $command eq "stop" || $command eq "stopssh" ) {
 
        my $exit_code = 1;
        eval {
            print "Disabling the VIP on old master: $orig_master_host \n";
            &stop_vip();
            $exit_code = 0;
        };
        if ($@) {
            warn "Got Error: $@\n";
            exit $exit_code;
        }
        exit $exit_code;
    }
    elsif ( $command eq "start" ) {
 
        my $exit_code = 10;
        eval {
            print "Enabling the VIP - $vip on the new master - $new_master_host \n";
            &start_vip();
            $exit_code = 0;
        };
        if ($@) {
            warn $@;
            exit $exit_code;
        }
        exit $exit_code;
    }
    elsif ( $command eq "status" ) {
        print "Checking the Status of the script.. OK \n";
        exit 0;
    }
    else {
        &usage();
        exit 1;
    }
}
sub start_vip() {
    `ssh $ssh_user\@$new_master_host \" $ssh_start_vip \"`;
}
# A simple system call that disable the VIP on the old_master
sub stop_vip() {
    `ssh $ssh_user\@$orig_master_host \" $ssh_stop_vip \"`;
}
 
sub usage {
    print
    "Usage: master_ip_failover --command=start|stop|stopssh|status --orig_master_host=host --orig_master_ip=ip --orig_master_port=port --new_master_host=host --new_master_ip=ip --new_master_port=port\n";
}

~~~

给自动切换脚本权限

~~~shell
chmod 755 /usr/local/bin/master_ip_failover
~~~

通过masterha_check_repl脚本查看整个集群的状态，出现 MySQL Replication Health is OK. 则表示成功
~~~shell
masterha_check_repl --conf=/etc/masterha/app1.cnf
~~~

检查MHA Manager的状态  ，如果正常，会显示"PING_OK"，否则会显示"NOT_RUNNING"，这代表MHA监控没有开启

~~~shell
masterha_check_status --conf=/etc/masterha/app1.cnf
~~~

开启MHA Manager监控

~~~~shell
nohup masterha_manager --conf=/etc/masterha/app1.cnf --remove_dead_master_conf --ignore_last_failover < /dev/null > /var/log/masterha/app1/manager.log 2>&1 &
~~~~

启动参数解释：

--remove_dead_master_conf   该参数代表当发生主从切换后，老的主库的IP将会从配置文件中移除

--manger_log         日志存放位置

--ignore_last_failover      在缺省情况下，如果MHA检测到连续发生宕机，且两次宕机间隔不足8小时的话，则不会进行Failover，之所以这样限制是为了避免ping-pong效应。该参数代表忽略上次MHA触发切换产生的文件，默认情况下，MHA发生切换后会在日志目录，也就是上面我设置的/data产生app1.failover.complete文件，下次再次切换的时候如果发现该目录下存在该文件将不允许触发切换，除非在第一次切换后删除该文件，为了方便，这里设置为--ignore_last_failover



检查MHA Manager的状态 ，出现 app1 (pid:3381) is running(0:PING_OK), master:192.168.25.139  表示成功

~~~shell
masterha_check_status --conf=/etc/masterha/app1.cnf
~~~

关闭MHA Manage监控

~~~shell
masterha_stop --conf=/etc/masterha/app1.cnf
~~~



### 部署java项目

#### 安装maven

~~~shell
#上传并解压maven源码包
tar -xf /tmp/apache-maven-3.9.4-bin.tar.gz -C /usr/local
#更名便于操作
mv /usr/local/apache-maven-3.9.4 /usr/local/maven
~~~

配置环境变量

~~~shell
vim /etc/profile.d/maven.sh

#! /bin/bash
export MAVEN_HOME=/usr/local/maven
export PATH=$MAVEN_HOME/bin:$PATH
~~~

~~~shell
source /etc/profile
~~~

查看maven版本

~~~~shell
mvn -v

Apache Maven 3.9.4 (dfbb324ad4a7c8fb0bf182e6d91b0ae20e3d2dd9)
Maven home: /usr/local/maven-3.9.4
Java version: 1.8.0_201, vendor: Oracle Corporation, runtime: /opt/jdk1.8/jre
Default locale: en_US, platform encoding: UTF-8
OS name: "linux", version: "3.10.0-693.el7.x86_64", arch: "amd64", family: "unix"
~~~~

配置maven加速器

~~~shell
#修改镜像
vim /usr/local/maven/conf/settings.xml 
#源镜像在文件约160行
#注释掉或删掉原来的镜像并改成下面阿里的镜像
<mirror>
    <id>aliyunmaven</id>
    <mirrorOf>*</mirrorOf>
    <name>阿里云公共仓库</name>
    <url>https://maven.aliyun.com/repository/public</url>
</mirror>
~~~

#### 配置java项目

~~~shell
tar -xf /tmp/myweb.tar.gz
cd mybeb
~~~

~~~shell
vim pom.xml

#在<project></project>里添加
<packaging>war</packaging>
~~~

~~~shell
vim /tmp/myweb/src/main/resources/application.properties

spring.datasource.driver-class-name=com.mysql.jdbc.Driver
spring.datasource.url=jdbc:mysql://192.168.25.106:8066/TESTDB1?characterEncoding=UTF-8
spring.datasource.username=root
spring.datasource.password=Ckc_123123

spring.jpa.hibernate.ddl-auto=update
spring.jpa.show-sql=true
~~~

使用maven将java项目打成war包

~~~shell
cd /tmp/myweb
mvn clean package -DskipTests
~~~

 将war包分配到集群

~~~shell
cp /tmp/myweb/targetmyweb-0.0.1-SNAPSHOT.war /opt/tomcat-8.5/webapps/ROOT.war
scp /opt/tomcat-8.5/webapps/ROOT.war root@192.168.25.135:/opt/tomcat-8.5/webapps
scp /opt/tomcat-8.5/webapps/ROOT.war root@192.168.25.136:/opt/tomcat-8.5/webapps
~~~

重启tomcat集群

~~~shell
tomcat restart
~~~



### zabbix监控

zabbix的yum源

~~~shell
rpm -Uvh https://repo.zabbix.com/zabbix/5.0/rhel/7/x86_64/zabbix-release-5.0-1.el7.noarch.rpm
#打开前端仓库
sed -i '11s/0/1/' /etc/yum.repos.d/zabbix.repo
~~~

安装zabbix前端和zabbix-server

~~~shell
yum install -y centos-release-scl
yum install -y zabbix-server-mysql zabbix-web-mysql-scl zabbix-apache-conf-scl
~~~

传sql到mysql139

~~~shell
scp /usr/share/doc/zabbix-server-mysql-5.0.39/create.sql.gz 192.168.25.139:/tmp
~~~

创建mysql用户

~~~mysql
mysql -uroot -puplooking_123
create database zabbix character set utf8 collate utf8_bin;
grant all on zabbix.* to 'zbx'@'%' identified by "Ckc_123123";
flush privileges;
#设置全局 运行创建函数
set global log_bin_trust_function_creators = 1;
~~~

导入数据

~~~shell
cd /tmp
zcat create.sql.gz | mysql -uzbx  -pCkc_123123 zabbix
~~~

还原MySQL设置

~~~shell
#关闭全局 不运行创建函数
mysql -uroot -pCkc_123123 -e "set global log_bin_trust_function_creators = 0;"
~~~

配置zabbix

~~~shell
vim /etc/zabbix/zabbix_server.conf

LogFile=/var/log/zabbix/zabbix_server.log
LogFileSize=0
PidFile=/var/run/zabbix/zabbix_server.pid
SocketDir=/var/run/zabbix
DBHost=192.168.25.139
DBName=zabbix
DBUser=zbx
DBPassword=Ckc_123123
SNMPTrapperFile=/var/log/snmptrap/snmptrap.log
Timeout=4
AlertScriptsPath=/usr/lib/zabbix/alertscripts
ExternalScripts=/usr/lib/zabbix/externalscripts
LogSlowQueries=3000
StatsAllowedIP=0.0.0.0
#监控JMX
JavaGateway=192.168.25.142
JavaGatewayPort=10052
StartJavaPollers=5
~~~

为Zabbix前端配置PHP 修改时区

~~~shell
yum install -y ntpdate
ntpdate ntp.aliyun.com
~~~

修改php配置

~~~shell
sed -i 's/; php_value\[date.timezone\] = Europe\/Riga/php_value\[date.timezone\] = Asia\/Shanghai/g' /etc/opt/rh/rh-php72/php-fpm.d/zabbix.conf
~~~

启动zabbix相关服务

~~~shell
systemctl restart zabbix-server httpd rh-php72-php-fpm
systemctl enable zabbix-server httpd rh-php72-php-fpm
~~~

访问zabbix页面进行初始化

http://192.168.25.142/zabbix

初始化完成后使用默认的账号密码进行登录

账号：Admin

密码：zabbix

登陆后在页面的左下角点击user settings，然后更改成中文

#### 监控主机

这里监控的主机均为**被动模式**

安装zabbix-agent监控自己

~~~shell
yum install -y zabbix-agent
~~~

配置agent

~~~~shell
vim /etc/zabbix/zabbix_agentd.conf

PidFile=/var/run/zabbix/zabbix_agentd.pid
LogFile=/var/log/zabbix/zabbix_agentd.log
LogFileSize=0
Server=127.0.0.1
ListenPort=10050
ListenIP=0.0.0.0
ServerActive=127.0.0.1
Hostname=zabbix142
Include=/etc/zabbix/zabbix_agentd.d/*.conf
~~~~

启动agent

~~~shell
systemctl start zabbix-agent.service
~~~

进到zabbix页面配置监控自己的agent

![](https://picdl.sunbangyan.cn/2023/11/18/51797e968956d74909f30a6392165faf.png)

分别在nginx，tomcat，mysql主机上安装agent2并监控

~~~shell
#同步时间
ntpdate ntp.aliyun.com 
#安装zabbix源
rpm -Uvh https://repo.zabbix.com/zabbix/5.0/rhel/7/x86_64/zabbix-release-5.0-1.el7.noarch.rpm
#安装agent2
yum install -y zabbix-agent2.x86_64
~~~

配置agent2

##### 监控nginx

~~~shell
vim /etc/zabbix/zabbix_agent2.conf

PidFile=/var/run/zabbix/zabbix_agent2.pid
LogType=file
LogFile=/var/log/zabbix/zabbix_agent2.log
LogFileSize=0
Server=192.168.25.142
ListenPort=10050
ListenIP=0.0.0.0
#注意这个改成对应的主机名
Hostname=nginx132
Include=/etc/zabbix/zabbix_agent2.d/*.conf
ControlSocket=/tmp/agent.sock
~~~

##### 监控tomcat

修改tomcat配置文件

~~~shell
vim /opt/tomcat-8.5/bin/catalina.sh

#在约335行处添加一下内容，hostname为tomcat主机的ip
CATALINA_OPTS="$CATALINA_OPTS -Djava.rmi.server.hostname=192.168.25.134     -Dcom.sun.management.jmxremote.port=1099     -Dcom.sun.management.jmxr    emote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false"
~~~

重启tomcat

~~~shell
tomcat restart
~~~

修改tomcat的agent配置文件

~~~shell
vim /etc/zabbix/zabbix_agent2.conf

PidFile=/var/run/zabbix/zabbix_agent2.pid
LogType=file
LogFile=/var/log/zabbix/zabbix_agent2.log
LogFileSize=0
Server=192.168.25.142
ListenPort=10050
ListenIP=0.0.0.0
#注意这个改成对应的主机名
Hostname=tomcat134
Include=/etc/zabbix/zabbix_agent2.d/*.conf
ControlSocket=/tmp/agent.sock
~~~

zabbix142上安装 Zabbix-java-gateway组件

~~~shell
yum install -y java java-devel zabbix-java-gateway
~~~

重启服务zabbix142

~~~shell
systemctl restart zabbix-server.service zabbix-java-gateway.service
systemctl enable zabbix-java-gateway.service
~~~

##### 监控MySQL

在MySQL master节点创建监控用户并授权权限

~~~
mysql -uroot -pCkc_123123 -e "CREATE USER 'zbx_monitor'@'%' IDENTIFIED BY 'Ckc_123123';GRANT REPLICATION CLIENT,PROCESS,SHOW DATABASES,SHOW VIEW ON *.* TO 'zbx_monitor'@'%';"
~~~

修改agent2配置文件

~~~shell
vim /etc/zabbix/zabbix_agent2.conf

PidFile=/var/run/zabbix/zabbix_agent2.pid
LogType=file
LogFile=/var/log/zabbix/zabbix_agent2.log
LogFileSize=0
Server=192.168.25.142
ListenPort=10050
ListenIP=0.0.0.0
#注意这个改成对应的主机名
Hostname=mysql139
Include=/etc/zabbix/zabbix_agent2.d/*.conf
ControlSocket=/tmp/agent.sock
~~~

最后全部启动anget2，并配置开机自启动

~~~shell
systemctl enable --now zabbix-agent2.service
~~~



所有主机都监控起来了

![](https://picss.sunbangyan.cn/2023/11/18/3e47bc42aacd4218b4a067004ac7365e.png)





### redis

下载redis源码包

~~~shell
cd /tmp
wget https://download.redis.io/releases/redis-7.0.13.tar.gz
~~~

解压源码包

~~~shell
tar xf redis-7.0.13.tar.gz -C /usr/local/
~~~

更名

~~~shell
mv /usr/local/redis-7.0.13 /usr/local/redis7
~~~

进入到redis目录

~~~shell
cd /usr/local/redis7
~~~

安装依赖

~~~shell
yum install -y gcc
~~~

编译

~~~shell
make
~~~

安装

~~~shell
make PREFIX=/usr/local/redis7 install
~~~

编辑配置文件

~~~shell
sed -i '87s/bind 127.0.0.1 -::1/bind 0.0.0.0/' /usr/local/redis7/redis.conf
sed -i '111s/yes/no/' /usr/local/redis7/redis.conf
sed -i '309s/no/yes/' /usr/local/redis7/redis.conf
~~~

使用systemd管理

~~~shell
vim /etc/systemd/system/redis.service

[unit]
Description=redis-server
After=network.target

[Service]
Type=forking
ExecStart=/usr/local/redis7/bin/redis-server /usr/local/redis7/redis.conf
PrivateTmp=true

[Install]
WantedBy=multi-user.target
~~~

软链接

~~~shell
ln -s /usr/local/redis7/bin/redis-cli /usr/bin/redis
~~~

启动redis

~~~shell
redis
~~~



### 日志搜集

使用filebeat6.8搜集主机的日志，并输出到redis

所有节点都安装filebeat

~~~shell
cd /tmp
curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.8.23-x86_64.rpm
rpm -ivh filebeat-6.8.22-x86_64.rpm
systemctl daemon-reload
~~~

备份原来filebeat配置文件

~~~shell
mv /etc/filebeat/filebeat.yml /etc/filebeat/filebeat.yml.bak
~~~

##### nginx

编写nginx的filebeat配置文件

~~~shell
cat > /etc/filebeat/filebeat.yml << EOF
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/nginx/*access.log*
  tags: ["nginx_access132"]
  exclude_files: ['.gz$','.zip','.bz2']
  json.keys_under_root: true
  json.overwrite_keys: true

- type: log
  enabled: true
  paths:
    - /var/log/nginx/*error.log*
  tags: ["nginx_error132"]
  exclude_files: ['.gz$','.zip','.bz2']

output.redis:
  hosts: ["192.168.25.143"]
  #password: ""
  port: 6379
  db: 0
  timeout: 5
  #注意这里的key用hostname，为了好区分
  key: "nginxdr132"
EOF
~~~

##### tomcat

编写tomcat的filebeat配置文件

~~~shell
vim /etc/filebeat/filebeat.yml

filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /opt/tomcat-8.5/logs/catalina.*.log
  tags: ['java134']
  exclude_files: ['.gz$']

  multiline.pattern: '^([0-9]{2}-[a-zA-Z]{3}-[0-9]{4})'
  multiline.negate: true
  multiline.match: after

- type: log
  enabled: true
  paths:
    - /opt/tomcat-8.5/logs/*.txt
  tags: ['tomcat134']
  json.keys_under_root: true
  json.overwrite_keys: true

output.redis:
  hosts: ["192.168.25.143"]
  #password: ""
  port: 6379
  db: 0
  timeout: 5
  #注意这里的key用hostname，为了好区分
  key: "tomcat134"
~~~

##### mycat

~~~shell
vim /etc/filebeat/filebeat.yml

filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /usr/local/mycat/logs/mycat.log
  #注意这里的tags用hostname，为了好区分
  tags: ['mycat137']

- type: log
  enabled: true
  paths:
    - /usr/local/mycat/logs/wrapper.log
  #注意这里的tags用hostname，为了好区分
  tags: ['wrapper137']

output.redis:
  hosts: ["192.168.25.143"]
  #password: ""
  port: 6379
  db: 0
  timeout: 5
  #注意这里的key用hostname，为了好区分
  key: "mycat137"
~~~

##### mysql

编写mysql的filebeat配置文件

~~~shell
vim /etc/filebeat/filebeat.yml

filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/mysqld.log
  #注意这里的tags用hostname，为了好区分
  tags: ['mysql139']

#output.file:
#  path: "/tmp"
#  filename: filebeat.txt
output.redis:
  hosts: ["192.168.25.143"]
  #password: ""
  port: 6379
  db: 0
  timeout: 5
  #注意这里的key用hostname，为了好区分
  key: "mysql139"

# 加载 module
#filebeat.config.modules:
#  path: ${path.config}/modules.d/*.yml
#  reload.enabled: true
#发现使用模块和直接读mysqld.log的内容是一致的
~~~

启动filebeat

~~~shell
systemctl enable --now filebeat.servicce
~~~



### logstash

配置jdk环境

~~~shell
tar xf /tmp/jdk-8u201-linux-x64.tar.gz -C /usr/local/
~~~

~~~shell
mv /usr/local/jdk1.8.0_201 /usr/local/jdk1.8
~~~

~~~shell
cat > /etc/profile.d/jdk.sh << EOF
JAVA_HOME=/usr/local/jdk1.8
JRE_HOME=$JAVA_HOME/jre
CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib
PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin
export JAVA_HOME JRE_HOME CLASSPATH
EOF
~~~

~~~shell
. /etc/profile
~~~

安装并配置logstash6.8.22

~~~shell
rpm -ivh /tmp/logstash-6.8.22.rpm
~~~

~~~shell
cat > /etc/profile.d/logstash.sh << EOF
#! /bin/bash
PATH=${PATH}:/usr/share/logstash/bin/
EOF
ln -s /usr/share/logstash/bin/logstash /usr/bin/
~~~

编写配置文件

~~~shell
cat > /etc/logstash/conf.d/frl.conf << EOF
input {
  redis {
    data_type => "list"
    db => 0
    host => "192.168.25.143"
    key => "nginxdr132"
    port => 6379
  }
}

input {
  redis {
    data_type => "list"
    db => 0
    host => "192.168.25.143"
    key => "nginx133"
    port => 6379
  }
}

input {
  redis {
    data_type => "list"
    db => 0
    host => "192.168.25.143"
    key => "tomcat134"
    port => 6379
  }
}

input {
  redis {
    data_type => "list"
    db => 0
    host => "192.168.25.143"
    key => "tomcat135"
    port => 6379
  }
}

input {
  redis {
    data_type => "list"
    db => 0
    host => "192.168.25.143"
    key => "tomcat136"
    port => 6379
  }
}

input {
  redis {
    data_type => "list"
    db => 0
    host => "192.168.25.143"
    key => "mycat137"
    port => 6379
  }
}

input {
  redis {
    data_type => "list"
    db => 0
    host => "192.168.25.143"
    key => "mycat138"
    port => 6379
  }
}

input {
  redis {
    data_type => "list"
    db => 0
    host => "192.168.25.143"
    key => "mysql139"
    port => 6379
  }
}

input {
  redis {
    data_type => "list"
    db => 0
    host => "192.168.25.143"
    key => "mysql140"
    port => 6379
  }
}

input {
  redis {
    data_type => "list"
    db => 0
    host => "192.168.25.143"
    key => "mysql141"
    port => 6379
  }
}


filter {
   grok {
      match => {
         "method" => "%{WORD:req} %{URIPATH:uri}"
      }
   }

   geoip {
       source => "cip"
       target => ["geoip"]
       fields => ["location","country_name","city_name"]
   }

   mutate {
      gsub => ["timestamp","[\[\]]",""]
      remove_field => ["input","beat","log","@version","prospector","offset","method"]
   }
   date {
      match => ["timestamp","dd/MMM/yyyy:HH:mm:ss Z"]
      target=> "@timestamp"
   }
}

output {
  stdout {}
  if "nginx_access132" in [tags]{
     elasticsearch {
        hosts => "192.168.25.145:9200"
        index => "nginxdr132-access-%{+yyyy.MM}"
      }
  }
    if "nginx_error132" in [tags]{
     elasticsearch {
        hosts => "192.168.25.145:9200"
        index => "nginxdr132-error-%{+yyyy.MM}"
      }
  }
    if "nginx_access133" in [tags]{
     elasticsearch {
        hosts => "192.168.25.145:9200"
        index => "nginxdr133-access-%{+yyyy.MM}"
      }
  }
    if "nginx_error133" in [tags]{
     elasticsearch {
        hosts => "192.168.25.145:9200"
        index => "nginxdr133-error-%{+yyyy.MM}"
      }
  }
    if "java134" in [tags]{
     elasticsearch {
        hosts => "192.168.25.145:9200"
        index => "tomcat134-java-%{+yyyy.MM}"
      }
  }
    if "tomcat134" in [tags]{
     elasticsearch {
        hosts => "192.168.25.145:9200"
        index => "tomcat134-tomcat-%{+yyyy.MM}"
      }
  }
    if "java135" in [tags]{
     elasticsearch {
        hosts => "192.168.25.145:9200"
        index => "tomcat135-java-%{+yyyy.MM}"
      }
  }
    if "tomcat135" in [tags]{
     elasticsearch {
        hosts => "192.168.25.145:9200"
        index => "tomcat135-tomcat-%{+yyyy.MM}"
      }
  }
    if "java136" in [tags]{
     elasticsearch {
        hosts => "192.168.25.145:9200"
        index => "tomcat136-java-%{+yyyy.MM}"
      }
  }
    if "tomcat136" in [tags]{
     elasticsearch {
        hosts => "192.168.25.146:9200"
        index => "tomcat135-tomcat-%{+yyyy.MM}"
      }
  }
    if "mycat137" in [tags]{
     elasticsearch {
        hosts => "192.168.25.145:9200"
        index => "mycat137-mycat-%{+yyyy.MM}"
      }
  }
    if "wrapper137" in [tags]{
     elasticsearch {
        hosts => "192.168.25.145:9200"
        index => "mycat137-wrapper-%{+yyyy.MM}"
      }
  }
    if "mycat138" in [tags]{
     elasticsearch {
        hosts => "192.168.25.145:9200"
        index => "mycat138-mycat-%{+yyyy.MM}"
      }
  }
    if "wrapper138" in [tags]{
     elasticsearch {
        hosts => "192.168.25.145:9200"
        index => "mycat138-wrapper-%{+yyyy.MM}"
      }
  }
    if "mysql139" in [tags]{
     elasticsearch {
        hosts => "192.168.25.145:9200"
        index => "mysql139-error--%{+yyyy.MM}"
      }
  }
    if "mysql140" in [tags]{
     elasticsearch {
        hosts => "192.168.25.145:9200"
        index => "mysql140-error--%{+yyyy.MM}"
      }
  }
    if "mysql141" in [tags]{
     elasticsearch {
        hosts => "192.168.25.145:9200"
        index => "mysql141-error--%{+yyyy.MM}"
      }
  }
}
EOF
~~~

启动logstash

~~~shell
#手动启动，且可以看到日志信息的输出
logstash -f /etc/logstash/conf.d/frl.conf
#后台启动
nohup logstash -f /etc/logstash/conf.d/frl.conf $> /dev/null &
~~~



### elasticsearch

配置jdk环境

~~~shell
tar xf /tmp/jdk-8u201-linux-x64.tar.gz -C /usr/local/
~~~

~~~shell
mv /usr/local/jdk1.8.0_201 /usr/local/jdk1.8
~~~

~~~shell
cat > /etc/profile.d/jdk.sh << EOF
JAVA_HOME=/usr/local/jdk1.8
JRE_HOME=$JAVA_HOME/jre
CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib
PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin
export JAVA_HOME JRE_HOME CLASSPATH
EOF
~~~

~~~shell
. /etc/profile
~~~

安装elasticsearch和kibana

~~~shell
rpm -ivh /tmp/elasticsearch-6.8.22.rpm
rpm -ivh /tmp/kibana-6.8.22-x86_64.rpm
~~~

~~~shell
systemctl daemon-reload
~~~

#### 优化系统

调整文件描述符限制

~~~shell
vim /etc/security/limits.conf

elasticsearch   soft    nofile  65535
elasticsearch   hard    nofile  65535
~~~

解除进程数限制

~~~
vim /etc/security/limits.d/20-nproc.conf

*          soft    nproc     65535
~~~

设置一个进程最大可使用虚拟内存的size

~~~shell
vim /etc/sysctl.conf

vm.max_map_count = 655360
~~~

使优化系统生效

~~~shell
 sysctl -p
~~~

配置elasticsearch

~~~shell
vim /etc/sysconfig/elasticsearch
#约第9行
JAVA_HOME=/usr/local/jdk1.8
~~~

~~~shell
vim /etc/elasticsearch/elasticsearch.yml

node.name: elasticsearch145
path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch
network.host: 192.168.25.145
http.port: 9200
~~~

JVM设置 内存限制

~~~shell
vim /etc/elasticsearch/jvm.options

#看情况修改
-Xms2g
-Xmx2g
~~~

配置kibana

~~~shell
vim /etc/kibana/kibana.yml
server.port: 5601
server.host: "192.168.25.145"
server.name: "elasticsearch145"
elasticsearch.hosts: ["http://192.168.25.145:9200"]
kibana.index: ".kibana"
~~~

启动elasticsearch和kibana

~~~shell
systemctl start elasticsearch.service
systemctl start kibana.service
~~~

kibana收集到的日志信息

![](https://picdl.sunbangyan.cn/2023/11/21/f0ad55ba6393dd1ed08f2ba9a909a3b6.jpeg)







### 总结

​      原本还想使用Jenkins和Gitlab进行CI/CD的，但是奈何电脑内存顶不住，所有没有使用到CI/CD，后续应该会出一个Jenkins+Gitlab进行CI/CD。

​      Mycat没有进行分库分表，只是进行读写分离而已。

​      Logstash的日志过滤也还没有写全，只做了@timestamp日志收集时间替换成了日志生成时间。

​      还有就是Nginx和Mycat应该做一个Keepalived以达到高可用的效果。



