---
layout: post
title: 收集nginx日志
subtitle: 使用filebeat+kafka+logstash
categories: Linux
author: FatGuy010
permalink: /2023/02/05/CollectNginxLogs
tags: [Linux]
---



## 使用filebeat+kafka+logstash收集nginx日志

### 图示

![](https://i.postimg.cc/8zsz6HFw/lkf.jpg)

### 环境

| host           | ip             |
| -------------- | -------------- |
| nginx-filebeat | 192.168.25.139 |
| nginx-filebeat | 192.168.25.140 |
| kafka          | 192.168.25.151 |
| kafka          | 192.168.25.152 |
| kafka          | 192.168.25.153 |
| logstash       | 192.168.25.150 |

### A记录解析

~~~shell
cat > /etc/hosts << EOF
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

192.168.25.139 mysql139
192.168.25.140 server140
192.168.25.150 server150
192.168.25.151 server151
192.168.25.152 jumpserver152
192.168.25.153 server153
EOF
~~~

### nginx-filebeat

配置nginx源

~~~shell
vim /etc/yum.repos.d/nginx.repo

[nginx-stable]
name=nginx stable repo
baseurl=http://nginx.org/packages/centos/$releasever/$basearch/
gpgcheck=0
enabled=1
gpgkey=https://nginx.org/keys/nginx_signing.key
module_hotfixes=true
~~~

安装nginx

~~~shell
yum install -y nginx
~~~

安装filebeat

附上官方地址：https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.8.0-x86_64.rpm

~~~shell
yum localinstall -y /tmp/filebeat-6.8.22-x86_64.rpm
~~~

nginx主配置文件

~~~shell
vim /etc/nginx/nginx.conf

user  nginx;
worker_processes  auto;
error_log  /var/log/nginx/error.log notice;
pid        /var/run/nginx.pid;

events {
    worker_connections  1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

log_format  main   '{"remote_addr":"$remote_addr","remote_user":"$remote_user",'
                   '"time_local":"$time_local","request":"$request","status":"$status",'
                   '"body_size":"$body_bytes_sent","referer":"$http_referer","agent":"$http_user_agent",'
                   '"x_forwarded":"$http_x_forwarded_for"}';

    access_log  /var/log/nginx/access.log  main;
    sendfile        on;
    keepalive_timeout  65;
    include /etc/nginx/conf.d/*.conf;
}
~~~

nginx子配置文件

~~~shell
vim /etc/nginx/conf.d/default.conf

server {
    listen       80;
    server_name  localhost;
    access_log  /var/log/nginx/web.access.log  main;

    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }
}
~~~

启动nginx

~~~shell
systemctl restart nginx
~~~

filebeat配置文件

~~~shell
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/nginx/*access.log*
  tags: ["nginx_access"]
  exclude_files: ['.gz$','.zip','.bz2']
  json.keys_under_root: true
  json.overwrite_keys: true

output.kafka:
  hosts:
    - 192.168.25.151:9092
    - 192.168.25.152:9092
    - 192.168.25.153:9092
  topic: nginx
  required_acks: 1
  broker_timeout: 3
  max_message_bytes: 102400000
  compression: "gzip"
  max_retries: 3
  worker: 3
  partition.round_robin:
    reachable_only: false

~~~

启动filebeat

~~~shell
systemctl restart filebeat.service
~~~



### kafka集群

配置jdk

~~~shell
tar -xf /tmp/jdk-8u201-linux-x64.tar.gz -C /usr/local/
~~~

~~~shell
mv /usr/local/jdk1.8.0_201/ /usr/local/jdk1.8
~~~

配置jdk环境变量

~~~shell
vim /etc/profile.d/jdk1.8.sh

#! /bin/bash
export JAVA_HOME=/usr/local/jdk1.8
export CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib
PATH=$JAVA_HOME/bin:$PATH
~~~

~~~shell
. /etc/profile
~~~

安装kafka

附上官方地址：https://archive.apache.org/dist/kafka/2.5.0/kafka_2.13-2.5.0.tgz

~~~shell
tar -xf /tmp/kafka_2.13-2.5.0.tgz -C /usr/local/
~~~

~~~shell
mv /usr/local/kafka_2.13-2.5.0 /usr/local/kafka
~~~

配置kafka环境变量

~~~shell
vim /etc/profile.d/kafka.sh

#! /bin/bash
PATH=/usr/local/kafka/bin/:${PATH}
~~~

~~~shell
source /etc/profile
~~~

修改zookeeper配置文件

~~~shell
vim /usr/local/kafka/config/zookeeper.properties

dataDir=/tmp/zookeeper
clientPort=2181
maxClientCnxns=0
conflicts.
admin.enableServer=false
tickTime=2000
initLimit=5
syncLimit=5
server.1=192.168.25.151:2888:3888
server.2=192.168.25.152:2888:3888
server.3=192.168.25.153:2888:3888
~~~

根据zookeeper配置文件的“server.1/2/3” 写入myid

~~~shell
mkdir -p /tmp/zookeeper
~~~

 ~~~shell
 echo 1 > /tmp/zookeeper/myid
 ~~~

~~~shell
echo 2 > /tmp/zookeeper/myid
~~~

~~~shell
echo 3 > /tmp/zookeeper/myid
~~~

启动zookeeper

~~~shell
zookeeper-server-start.sh -daemon /usr/local/kafka/config/zookeeper.properties
~~~

kadka配置文件

~~~shell
vim /usr/local/kafka/config/server.properties

#id唯一
broker.id=151
#修改成对应的host
listeners=PLAINTEXT://192.168.25.151:9092
num.network.threads=3
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
log.dirs=/tmp/kafka-logs
num.partitions=1
num.recovery.threads.per.data.dir=1
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1
log.retention.hours=168
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000
#添加集群
zookeeper.connect=192.168.25.151:2181,192.168.25.152:2181,192.168.25.153:2181
zookeeper.connection.timeout.ms=18000
group.initial.rebalance.delay.ms=0
~~~

启动kafka

~~~shell
nohup /usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties &>> /usr/local/kafka/logs/kafka.log &
~~~

创建kafka的topic主题

~~~shell
kafka-topics.sh --bootstrap-server server151:9092,server153:9092,jumpserver152:9092 --create --topic nginx --partitions 3 --replication-factor 3 --config  cleanup.policy=delete --config retention.ms=36000000
~~~

查看创建的topic详细信息

~~~shell
kafka-topics.sh --bootstrap-server server151:9092,server153:9092,jumpserver152:9092 --describe
~~~

查看传来的数据

~~~shell
kafka-console-consumer.sh --bootstrap-server 192.168.25.141:9092 --topic nginx --from-beginning
~~~



### logstash

配置jdk环境

~~~shell
tar -xf /tmp/jdk-8u201-linux-x64.tar.gz -C /usr/local/
~~~

~~~shell
mv /usr/local/jdk1.8.0_201 /usr/local/jdk1.8
~~~

~~~~shell
vim /etc/profile.d/jdk.sh

JAVA_HOME=/usr/local/jdk1.8
JRE_HOME=$JAVA_HOME/jre
PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin
CLASSPATH=$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib
export JAVA_HOME JRE_HOME CLASSPATH
~~~~

~~~shell
. /etc/profile
~~~

安装logstash

附上官方地址：https://artifacts.elastic.co/downloads/logstash/logstash-6.8.0.rpm

~~~shell
rpm -ivh /tmp/logstash-6.8.22.rpm
~~~

配置环境变量

~~~shell
vim /etc/profile.d/logstash.sh

#! /bin/bash
PATH=${PATH}:/usr/share/logstash/bin/
~~~

制作软链接

~~~shell
ln -s /usr/share/logstash/bin/logstash /usr/bin/
~~~

编写logstash配置文件

~~~shell
input {
  kafka {
    bootstrap_servers => ["192.168.25.151:9092,192.168.25.152:9092,192.168.25.153:9092"]
    group_id => "nginx"
    topics => ["nginx"]
    codec => json
    consumer_threads => 3
    decorate_events => false
 }
}

filter {
   geoip {
       source => "cip"
       target => ["geoip"]
       fields => ["location","country_name","city_name"]
   }

   mutate {
      remove_field => ["input","beat","log","@version","prospector","offset","method"]
   }
   date {
      match => ["timestamp","dd/MMM/yyyy:HH:mm:ss Z"]
      target=> "@timestamp"
   }
}

output {
    stdout {}
}
~~~

启动logstash在前台显示

~~~shell
logstash -f /etc/logstash/conf.d/lkf.conf
~~~



多实例

创建数据目录

~~~shell
mkdir -p /var/lib/logstash/data{1..10}
~~~

~~~shell
 chown -R logstash:logstash /var/lib/logstash/
~~~

指定不同的数据目录启动多实例

~~~shell
logstash -f /etc/logstash/conf.d/lkf.conf --path.data /var/lib/logstash/data1
~~~

~~~~shell
logstash -f /etc/logstash/conf.d/lkf.conf --path.data /var/lib/logstash/data2
~~~~

~~~shell
logstash -f /etc/logstash/conf.d/lkf.conf --path.data /var/lib/logstash/data3
~~~

以此类推



后续可以在继续加上elasticsearch集群和kibana可视化界面，这里暂时直接输出到控制台
